{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f0e87",
   "metadata": {},
   "source": [
    "# Final project code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "239371b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "import gensim\n",
    "import nltk\n",
    "import string \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "965f64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1fe6d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_jsons/apple.json', 'r') as f:\n",
    "    apple_data = json.load(f)\n",
    "with open('training_jsons/google.json', 'r') as f:\n",
    "    google_data = json.load(f)\n",
    "with open('training_jsons/microsoft.json', 'r') as f:\n",
    "    microsoft_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "75b609ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = apple_data + google_data + microsoft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34304291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offer_status': 'No Offer', 'experience': 'Negative Experience', 'difficulty': 'Easy Interview', 'review': '10 minute speed run, very abrupt with unengaged interviewers. Asked behavioral questions and about previous projects. Describe a previous coding project, interests, Why Apple? \\n\\nDid not find this format engaging, thought I do like the efficiency of the structure. Interviewers did not seem to care or have enough time to get to know candidates', 'page': 120}\n"
     ]
    }
   ],
   "source": [
    "print(data[232]) #Remove \\r\\n(-: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "991a3731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Declined Offer', 'No Offer', 'Accepted Offer'}\n",
      "{'Neutral Experience', 'Positive Experience', 'Negative Experience'}\n",
      "{'Average Interview', 'Easy Interview', 'Difficult Interview'}\n"
     ]
    }
   ],
   "source": [
    "#Types of offers\n",
    "offer = set()\n",
    "experience = set()\n",
    "difficulty = set()\n",
    "for i in data:\n",
    "    offer.add(i[\"offer_status\"])\n",
    "    experience.add(i[\"experience\"])\n",
    "    difficulty.add(i[\"difficulty\"])\n",
    "print(offer)\n",
    "print(experience)\n",
    "print(difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5803c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "{'experience': 'Positive Experience', 'difficulty': 'Average Interview', 'review': 'initial phone interview 15mins received an email 2 days after applying inviting me to schedule a time to call with an apple retail recruiter who was based in california applying for job in texas was told at the end of the phone call that i would be advancing to the next step invited to an optional get to know apple web event 30mins which went over some basics about working in apple retail and company culturegroup interview 45mins group interview with manager from the apple store i was applying to and 3 other interviewees 4 interviewees total took place online via their webex platform similar to zoom as of aug 2021 most interviewees were online 105 minutes before the interview was scheduled to begin and an icebreaker question was asked about a minute before the scheduled start time the invite email suggested we use a digital background for our own privacy most felt comfortable without it a question would be asked and we would each take turns answering in any order we pleased questions were situationbased', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "X = []\n",
    "#0 represents no offer\n",
    "#1 represents declined offer or accepted offer\n",
    "#Decide if we should predict offer or the experience?\n",
    "#Preprocessing and creating y_matrix\n",
    "\n",
    "#Future step: Tokenize to remove stop words to only include stems of words?\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    #Remove \\w \\s \\n\n",
    "    data[i][\"review\"] = data[i][\"review\"].strip().replace('\\n', '').replace('\\r', '').lower()\n",
    "    data[i][\"review\"] = re.sub(r'[^\\w\\s\\n\\r]', '', data[i][\"review\"]).strip()\n",
    "    #Remove links\n",
    "    data[i][\"review\"] = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', data[i][\"review\"])\n",
    "    if data[i][\"offer_status\"] == 'No Offer':\n",
    "        data[i][\"offer_status\"] = 0\n",
    "        y.append(0)\n",
    "    else:\n",
    "        data[i][\"offer_status\"] = 1\n",
    "        y.append(1)\n",
    "    del data[i]['offer_status']\n",
    "print(y[1:10])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "715e23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "afa98be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f84a709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/dinakartalluri/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Define pretrained model\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e00d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "\n",
    "#Create features for each review\n",
    "#First interation\n",
    "#Interview experience (numeric), difficulty (numeric), avg word embedding for review? (use glove embedding), # of words added to each axis(pick valid threshold)(divide by length of text) (future), use pretrained classifiers (next step)\n",
    "def extract_features(embed, data, include, sid):\n",
    "    X_features = np.zeros((len(data), 4))\n",
    "    if not include:\n",
    "        X_features = np.zeros((len(data), 3))\n",
    "    for i, d in enumerate(data):\n",
    "        feature_1 = 0\n",
    "        feature_2 = 0\n",
    "\n",
    "        if d[\"experience\"] == \"Positive Experience\":\n",
    "            feature_1 = 1\n",
    "        elif d[\"experience\"] == \"Negative Experience\":\n",
    "            feature_1 = -1\n",
    "\n",
    "        if d[\"difficulty\"] == \"Easy Interview\":\n",
    "            feature_2 = 1\n",
    "        elif d[\"difficulty\"] == \"Difficult Interview\":\n",
    "            feature_2 = -1\n",
    "        \n",
    "        tokenized = word_tokenize(d[\"review\"])\n",
    "        stopwords_english = stopwords.words('english')\n",
    "        \n",
    "        feature_3 = 0\n",
    "        number_embed = 0\n",
    "        for token in tokenized:\n",
    "            if token not in stopwords_english and token not in string.punctuation and token[0].isdigit() == False:\n",
    "                if token in embed:\n",
    "                    feature_3+=embed[token]\n",
    "                    number_embed+=1\n",
    "        feature_3/=number_embed\n",
    "        feature_3 = np.array(feature_3).mean()\n",
    "        \n",
    "        out = sid.polarity_scores(d[\"review\"])\n",
    "        feature_4 = out['compound']\n",
    "        \n",
    "        X_features[i] = np.array([feature_1, feature_2, feature_3, feature_4])\n",
    "    return X_features\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78e2637e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000000e+00, 0.000000e+00, 5.524726e-04, 2.732000e-01])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the features\n",
    "\n",
    "X_train_features_with_i = extract_features(embed, X_train, True, sid)\n",
    "X_test_features_with_i = extract_features(embed, X_test, True, sid)\n",
    "X_train_features_with_i[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "836376f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.6705748259098674\n",
      "best parameters  {'C': 0.01, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "#fit LR, tuning\n",
    "params={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "lr=LogisticRegression(solver='liblinear')\n",
    "lr_cv=GridSearchCV(lr,params,cv=10)\n",
    "lr_cv.fit(X_train_features_with_i,y_train)\n",
    "print(\"accuracy :\",lr_cv.best_score_)\n",
    "print(\"best parameters \",lr_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "09b52758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6205408684421984\n",
      "0.6598151062155783\n",
      "0.6598151062155783\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression(C=0.01,penalty=\"l2\")\n",
    "lr.fit(X_train_features_with_i,y_train)\n",
    "y_pred = lr.predict(X_test_features_with_i)\n",
    "print(f1_score(y_test, y_pred, average='weighted'))\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd9fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
